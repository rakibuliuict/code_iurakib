[10:04:23.281] Namespace(root_path='/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI', exp='SDCL', model='VNet', pre_max_iteration=2000, self_max_iteration=15000, max_samples=80, labeled_bs=4, batch_size=8, base_lr=0.001, deterministic=1, labelnum=8, gpu='0', seed=1345, consistency=1.0, consistency_rampup=40.0, magnitude=10.0, u_weight=0.5, mask_ratio=0.6666666666666666, u_alpha=2.0, loss_weight=0.5)
[10:04:24.015] train_lab set: total 8 samples
[10:04:24.015] total ['/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/06SR5RBREL16DQ6M8LWS/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/0RZDK210BSMWAA6467LU/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1D7CUD1955YZPGK8XHJX/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1GU15S0GJ6PFNARO469W/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1MHBF3G6DCPWHSKG7XCP/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/23X6SY44VT9KFHR7S7OC/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/2XL5HSFSE93RMOJDRGR4/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/38CWS74285MFGZZXR09Z/mri_norm2.h5'] samples
[10:04:24.019] train_lab set: total 8 samples
[10:04:24.019] total ['/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/06SR5RBREL16DQ6M8LWS/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/0RZDK210BSMWAA6467LU/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1D7CUD1955YZPGK8XHJX/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1GU15S0GJ6PFNARO469W/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/1MHBF3G6DCPWHSKG7XCP/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/23X6SY44VT9KFHR7S7OC/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/2XL5HSFSE93RMOJDRGR4/mri_norm2.h5', '/content/drive/MyDrive/SemiSL/Dataset/2018_UTAH_MICCAI/Training Set/38CWS74285MFGZZXR09Z/mri_norm2.h5'] samples
[10:04:24.022] 40 iterations per epoch
[10:04:24.024] 

[10:04:26.167] iteration 1 : loss: 0.809023, loss_dice: 0.619381, loss_ce: 0.998665
[10:04:27.679] iteration 2 : loss: 0.776670, loss_dice: 0.597845, loss_ce: 0.955496
[10:04:29.151] iteration 3 : loss: 0.785931, loss_dice: 0.565157, loss_ce: 1.006706
[10:04:30.624] iteration 4 : loss: 0.665670, loss_dice: 0.514858, loss_ce: 0.816481
[10:04:32.085] iteration 5 : loss: 0.720878, loss_dice: 0.552512, loss_ce: 0.889245
[10:04:33.558] iteration 6 : loss: 0.626831, loss_dice: 0.535774, loss_ce: 0.717888
[10:04:35.015] iteration 7 : loss: 0.655470, loss_dice: 0.538879, loss_ce: 0.772060
[10:04:36.504] iteration 8 : loss: 0.700820, loss_dice: 0.576487, loss_ce: 0.825153
[10:04:38.061] iteration 9 : loss: 0.647019, loss_dice: 0.522661, loss_ce: 0.771378
[10:04:39.630] iteration 10 : loss: 0.615736, loss_dice: 0.522275, loss_ce: 0.709197
[10:04:41.099] iteration 11 : loss: 0.630263, loss_dice: 0.535606, loss_ce: 0.724919
[10:04:42.570] iteration 12 : loss: 0.670949, loss_dice: 0.563697, loss_ce: 0.778202
[10:04:44.068] iteration 13 : loss: 0.626526, loss_dice: 0.531732, loss_ce: 0.721320
[10:04:45.528] iteration 14 : loss: 0.584581, loss_dice: 0.524292, loss_ce: 0.644871
[10:04:46.995] iteration 15 : loss: 0.638324, loss_dice: 0.555519, loss_ce: 0.721130
[10:04:48.536] iteration 16 : loss: 0.634524, loss_dice: 0.556194, loss_ce: 0.712854
[10:04:49.994] iteration 17 : loss: 0.584055, loss_dice: 0.532808, loss_ce: 0.635303
[10:04:51.571] iteration 18 : loss: 0.666695, loss_dice: 0.567086, loss_ce: 0.766303
[10:04:53.144] iteration 19 : loss: 0.521363, loss_dice: 0.488654, loss_ce: 0.554071
[10:04:54.627] iteration 20 : loss: 0.535987, loss_dice: 0.489677, loss_ce: 0.582298
[10:04:56.104] iteration 21 : loss: 0.536015, loss_dice: 0.497616, loss_ce: 0.574414
[10:04:57.582] iteration 22 : loss: 0.583518, loss_dice: 0.525945, loss_ce: 0.641091
[10:04:59.057] iteration 23 : loss: 0.545214, loss_dice: 0.510769, loss_ce: 0.579659
[10:05:00.544] iteration 24 : loss: 0.544053, loss_dice: 0.498306, loss_ce: 0.589799
[10:05:02.027] iteration 25 : loss: 0.570813, loss_dice: 0.533525, loss_ce: 0.608101
[10:05:03.508] iteration 26 : loss: 0.537754, loss_dice: 0.500269, loss_ce: 0.575240
[10:05:05.088] iteration 27 : loss: 0.498025, loss_dice: 0.464363, loss_ce: 0.531688
[10:05:06.682] iteration 28 : loss: 0.592705, loss_dice: 0.538146, loss_ce: 0.647265
[10:05:08.164] iteration 29 : loss: 0.587886, loss_dice: 0.522553, loss_ce: 0.653219
[10:05:09.644] iteration 30 : loss: 0.520495, loss_dice: 0.500252, loss_ce: 0.540738
[10:05:11.122] iteration 31 : loss: 0.603025, loss_dice: 0.538469, loss_ce: 0.667581
[10:05:12.604] iteration 32 : loss: 0.530934, loss_dice: 0.509333, loss_ce: 0.552535
[10:05:14.099] iteration 33 : loss: 0.511278, loss_dice: 0.490907, loss_ce: 0.531649
[10:05:15.592] iteration 34 : loss: 0.507734, loss_dice: 0.491652, loss_ce: 0.523815
[10:05:17.190] iteration 35 : loss: 0.536078, loss_dice: 0.499046, loss_ce: 0.573110
[10:05:18.768] iteration 36 : loss: 0.516046, loss_dice: 0.470749, loss_ce: 0.561342
[10:05:20.265] iteration 37 : loss: 0.451570, loss_dice: 0.438101, loss_ce: 0.465038
[10:05:21.759] iteration 38 : loss: 0.538252, loss_dice: 0.488556, loss_ce: 0.587948
[10:05:23.260] iteration 39 : loss: 0.444494, loss_dice: 0.438989, loss_ce: 0.449998
[10:05:24.759] iteration 40 : loss: 0.502738, loss_dice: 0.463060, loss_ce: 0.542416
[10:05:24.767] 

[10:05:26.271] iteration 41 : loss: 0.369668, loss_dice: 0.360502, loss_ce: 0.378835
[10:05:27.780] iteration 42 : loss: 0.525261, loss_dice: 0.491476, loss_ce: 0.559047
[10:05:29.295] iteration 43 : loss: 0.460209, loss_dice: 0.443789, loss_ce: 0.476630
[10:05:30.886] iteration 44 : loss: 0.481459, loss_dice: 0.449945, loss_ce: 0.512974
[10:05:32.484] iteration 45 : loss: 0.579384, loss_dice: 0.519912, loss_ce: 0.638856
[10:05:33.984] iteration 46 : loss: 0.500483, loss_dice: 0.453964, loss_ce: 0.547001
[10:05:35.503] iteration 47 : loss: 0.508519, loss_dice: 0.486690, loss_ce: 0.530348
[10:05:37.020] iteration 48 : loss: 0.451762, loss_dice: 0.425508, loss_ce: 0.478016
[10:05:38.535] iteration 49 : loss: 0.501925, loss_dice: 0.461574, loss_ce: 0.542276
[10:05:40.034] iteration 50 : loss: 0.504351, loss_dice: 0.466321, loss_ce: 0.542380
[10:05:41.551] iteration 51 : loss: 0.535703, loss_dice: 0.487452, loss_ce: 0.583955
[10:05:43.131] iteration 52 : loss: 0.467291, loss_dice: 0.439529, loss_ce: 0.495053
[10:05:44.732] iteration 53 : loss: 0.390303, loss_dice: 0.384152, loss_ce: 0.396454
[10:05:46.290] iteration 54 : loss: 0.474041, loss_dice: 0.459052, loss_ce: 0.489031
[10:05:47.809] iteration 55 : loss: 0.433785, loss_dice: 0.427717, loss_ce: 0.439852
[10:05:49.329] iteration 56 : loss: 0.500658, loss_dice: 0.470226, loss_ce: 0.531089
[10:05:50.847] iteration 57 : loss: 0.540288, loss_dice: 0.488922, loss_ce: 0.591655
[10:05:52.383] iteration 58 : loss: 0.383081, loss_dice: 0.373316, loss_ce: 0.392845
[10:05:53.902] iteration 59 : loss: 0.573497, loss_dice: 0.511524, loss_ce: 0.635469
[10:05:55.439] iteration 60 : loss: 0.507754, loss_dice: 0.467392, loss_ce: 0.548115
[10:05:57.093] iteration 61 : loss: 0.430911, loss_dice: 0.407286, loss_ce: 0.454535
[10:05:58.755] iteration 62 : loss: 0.421285, loss_dice: 0.412535, loss_ce: 0.430035
[10:06:00.295] iteration 63 : loss: 0.396000, loss_dice: 0.402392, loss_ce: 0.389609
[10:06:01.829] iteration 64 : loss: 0.408336, loss_dice: 0.392893, loss_ce: 0.423779
[10:06:03.355] iteration 65 : loss: 0.447444, loss_dice: 0.439368, loss_ce: 0.455520
[10:06:04.884] iteration 66 : loss: 0.433442, loss_dice: 0.425367, loss_ce: 0.441517
[10:06:06.409] iteration 67 : loss: 0.521803, loss_dice: 0.478461, loss_ce: 0.565144
[10:06:07.959] iteration 68 : loss: 0.521286, loss_dice: 0.468890, loss_ce: 0.573682
[10:06:09.559] iteration 69 : loss: 0.517220, loss_dice: 0.481008, loss_ce: 0.553433
[10:06:11.184] iteration 70 : loss: 0.498435, loss_dice: 0.491558, loss_ce: 0.505311
[10:06:12.732] iteration 71 : loss: 0.505150, loss_dice: 0.440431, loss_ce: 0.569869
[10:06:14.274] iteration 72 : loss: 0.403098, loss_dice: 0.389200, loss_ce: 0.416996
[10:06:15.819] iteration 73 : loss: 0.325600, loss_dice: 0.322374, loss_ce: 0.328825
[10:06:17.369] iteration 74 : loss: 0.371274, loss_dice: 0.375825, loss_ce: 0.366723
[10:06:18.924] iteration 75 : loss: 0.398667, loss_dice: 0.400769, loss_ce: 0.396564
[10:06:20.478] iteration 76 : loss: 0.395835, loss_dice: 0.385384, loss_ce: 0.406286
[10:06:22.034] iteration 77 : loss: 0.383837, loss_dice: 0.387481, loss_ce: 0.380193
[10:06:23.681] iteration 78 : loss: 0.457006, loss_dice: 0.447059, loss_ce: 0.466953
[10:06:25.335] iteration 79 : loss: 0.412130, loss_dice: 0.411393, loss_ce: 0.412866
[10:06:26.993] iteration 80 : loss: 0.533593, loss_dice: 0.471139, loss_ce: 0.596047
[10:06:26.997] 

[10:06:28.650] iteration 81 : loss: 0.394834, loss_dice: 0.391109, loss_ce: 0.398559
[10:06:30.227] iteration 82 : loss: 0.445826, loss_dice: 0.427247, loss_ce: 0.464405
[10:06:31.784] iteration 83 : loss: 0.575721, loss_dice: 0.495181, loss_ce: 0.656261
[10:06:33.341] iteration 84 : loss: 0.385936, loss_dice: 0.378498, loss_ce: 0.393374
[10:06:34.898] iteration 85 : loss: 0.349445, loss_dice: 0.337033, loss_ce: 0.361857
[10:06:36.547] iteration 86 : loss: 0.413406, loss_dice: 0.424441, loss_ce: 0.402371
[10:06:38.213] iteration 87 : loss: 0.410751, loss_dice: 0.411522, loss_ce: 0.409980
[10:06:39.772] iteration 88 : loss: 0.429369, loss_dice: 0.425007, loss_ce: 0.433731
[10:06:41.339] iteration 89 : loss: 0.412751, loss_dice: 0.384578, loss_ce: 0.440923
[10:06:42.926] iteration 90 : loss: 0.406628, loss_dice: 0.388043, loss_ce: 0.425212
[10:06:44.482] iteration 91 : loss: 0.428531, loss_dice: 0.415259, loss_ce: 0.441803
[10:06:46.052] iteration 92 : loss: 0.381459, loss_dice: 0.386831, loss_ce: 0.376087
[10:06:47.618] iteration 93 : loss: 0.504758, loss_dice: 0.467501, loss_ce: 0.542015
[10:06:49.294] iteration 94 : loss: 0.349972, loss_dice: 0.336536, loss_ce: 0.363408
[10:06:50.933] iteration 95 : loss: 0.427669, loss_dice: 0.412029, loss_ce: 0.443310
[10:06:52.490] iteration 96 : loss: 0.457980, loss_dice: 0.426202, loss_ce: 0.489758
[10:06:54.053] iteration 97 : loss: 0.292081, loss_dice: 0.304425, loss_ce: 0.279737
[10:06:55.603] iteration 98 : loss: 0.379233, loss_dice: 0.391223, loss_ce: 0.367242
[10:06:57.146] iteration 99 : loss: 0.436070, loss_dice: 0.405367, loss_ce: 0.466772
[10:06:58.707] iteration 100 : loss: 0.378527, loss_dice: 0.368376, loss_ce: 0.388678
[10:07:00.244] iteration 101 : loss: 0.367766, loss_dice: 0.352724, loss_ce: 0.382809
[10:07:01.792] iteration 102 : loss: 0.429485, loss_dice: 0.410633, loss_ce: 0.448338
[10:07:03.420] iteration 103 : loss: 0.330056, loss_dice: 0.348938, loss_ce: 0.311175
[10:07:05.019] iteration 104 : loss: 0.360218, loss_dice: 0.368685, loss_ce: 0.351750
[10:07:06.584] iteration 105 : loss: 0.397197, loss_dice: 0.387598, loss_ce: 0.406796
[10:07:08.123] iteration 106 : loss: 0.401656, loss_dice: 0.400151, loss_ce: 0.403160
[10:07:09.676] iteration 107 : loss: 0.384917, loss_dice: 0.375720, loss_ce: 0.394113
[10:07:11.221] iteration 108 : loss: 0.431217, loss_dice: 0.418106, loss_ce: 0.444328
[10:07:12.769] iteration 109 : loss: 0.341880, loss_dice: 0.346926, loss_ce: 0.336834
[10:07:14.309] iteration 110 : loss: 0.562885, loss_dice: 0.513236, loss_ce: 0.612533
[10:07:15.967] iteration 111 : loss: 0.375047, loss_dice: 0.363700, loss_ce: 0.386394
[10:07:17.606] iteration 112 : loss: 0.312115, loss_dice: 0.335032, loss_ce: 0.289197
[10:07:19.164] iteration 113 : loss: 0.362984, loss_dice: 0.372350, loss_ce: 0.353618
[10:07:20.704] iteration 114 : loss: 0.331815, loss_dice: 0.345587, loss_ce: 0.318043
[10:07:22.247] iteration 115 : loss: 0.359834, loss_dice: 0.363802, loss_ce: 0.355865
[10:07:23.786] iteration 116 : loss: 0.416516, loss_dice: 0.397432, loss_ce: 0.435601
[10:07:25.345] iteration 117 : loss: 0.317075, loss_dice: 0.330719, loss_ce: 0.303431
[10:07:26.885] iteration 118 : loss: 0.383026, loss_dice: 0.373541, loss_ce: 0.392512
[10:07:28.548] iteration 119 : loss: 0.335143, loss_dice: 0.334165, loss_ce: 0.336122
[10:07:30.187] iteration 120 : loss: 0.389765, loss_dice: 0.355833, loss_ce: 0.423696
[10:07:30.188] 

[10:07:31.725] iteration 121 : loss: 0.346323, loss_dice: 0.340179, loss_ce: 0.352466
[10:07:33.305] iteration 122 : loss: 0.341714, loss_dice: 0.353746, loss_ce: 0.329682
[10:07:34.840] iteration 123 : loss: 0.487321, loss_dice: 0.451607, loss_ce: 0.523036
[10:07:36.378] iteration 124 : loss: 0.340834, loss_dice: 0.332842, loss_ce: 0.348826
[10:07:37.916] iteration 125 : loss: 0.389615, loss_dice: 0.389505, loss_ce: 0.389725
[10:07:39.475] iteration 126 : loss: 0.459318, loss_dice: 0.421121, loss_ce: 0.497515
[10:07:41.019] iteration 127 : loss: 0.355929, loss_dice: 0.367170, loss_ce: 0.344687
[10:07:42.678] iteration 128 : loss: 0.412649, loss_dice: 0.413441, loss_ce: 0.411857
[10:07:44.351] iteration 129 : loss: 0.308879, loss_dice: 0.307397, loss_ce: 0.310360
[10:07:45.910] iteration 130 : loss: 0.349354, loss_dice: 0.351938, loss_ce: 0.346769
[10:07:47.467] iteration 131 : loss: 0.392346, loss_dice: 0.376109, loss_ce: 0.408584
[10:07:49.021] iteration 132 : loss: 0.330718, loss_dice: 0.342476, loss_ce: 0.318959
[10:07:50.560] iteration 133 : loss: 0.365957, loss_dice: 0.373629, loss_ce: 0.358285
[10:07:52.118] iteration 134 : loss: 0.380899, loss_dice: 0.388872, loss_ce: 0.372927
[10:07:53.670] iteration 135 : loss: 0.366681, loss_dice: 0.355070, loss_ce: 0.378292
[10:07:55.313] iteration 136 : loss: 0.382690, loss_dice: 0.372825, loss_ce: 0.392555
[10:07:56.957] iteration 137 : loss: 0.308609, loss_dice: 0.320900, loss_ce: 0.296318
[10:07:58.525] iteration 138 : loss: 0.303738, loss_dice: 0.313139, loss_ce: 0.294337
[10:08:00.070] iteration 139 : loss: 0.427995, loss_dice: 0.420681, loss_ce: 0.435309
[10:08:01.629] iteration 140 : loss: 0.391640, loss_dice: 0.384879, loss_ce: 0.398401
[10:08:03.187] iteration 141 : loss: 0.355637, loss_dice: 0.345494, loss_ce: 0.365780
[10:08:04.750] iteration 142 : loss: 0.318054, loss_dice: 0.332609, loss_ce: 0.303499
[10:08:06.304] iteration 143 : loss: 0.356972, loss_dice: 0.360878, loss_ce: 0.353066
[10:08:07.945] iteration 144 : loss: 0.341566, loss_dice: 0.334627, loss_ce: 0.348504
[10:08:09.588] iteration 145 : loss: 0.253929, loss_dice: 0.283911, loss_ce: 0.223946
[10:08:11.136] iteration 146 : loss: 0.351277, loss_dice: 0.371183, loss_ce: 0.331371
[10:08:12.695] iteration 147 : loss: 0.416926, loss_dice: 0.397084, loss_ce: 0.436769
[10:08:14.246] iteration 148 : loss: 0.301432, loss_dice: 0.286187, loss_ce: 0.316677
[10:08:15.817] iteration 149 : loss: 0.319936, loss_dice: 0.328455, loss_ce: 0.311418
[10:08:17.373] iteration 150 : loss: 0.330764, loss_dice: 0.331098, loss_ce: 0.330431
[10:08:18.939] iteration 151 : loss: 0.346135, loss_dice: 0.341172, loss_ce: 0.351098
[10:08:20.497] iteration 152 : loss: 0.316836, loss_dice: 0.312355, loss_ce: 0.321317
[10:08:22.143] iteration 153 : loss: 0.245257, loss_dice: 0.263362, loss_ce: 0.227153
[10:08:23.720] iteration 154 : loss: 0.258853, loss_dice: 0.266672, loss_ce: 0.251034
[10:08:25.274] iteration 155 : loss: 0.246864, loss_dice: 0.262463, loss_ce: 0.231266
[10:08:26.825] iteration 156 : loss: 0.275841, loss_dice: 0.283954, loss_ce: 0.267728
[10:08:28.370] iteration 157 : loss: 0.341883, loss_dice: 0.324661, loss_ce: 0.359105
[10:08:29.926] iteration 158 : loss: 0.326772, loss_dice: 0.332648, loss_ce: 0.320896
[10:08:31.495] iteration 159 : loss: 0.300512, loss_dice: 0.317835, loss_ce: 0.283188
[10:08:33.054] iteration 160 : loss: 0.247556, loss_dice: 0.250614, loss_ce: 0.244498
[10:08:33.055] 

[10:08:34.696] iteration 161 : loss: 0.263053, loss_dice: 0.260065, loss_ce: 0.266042
[10:08:36.337] iteration 162 : loss: 0.311962, loss_dice: 0.303132, loss_ce: 0.320792
[10:08:37.893] iteration 163 : loss: 0.306451, loss_dice: 0.299145, loss_ce: 0.313756
[10:08:39.466] iteration 164 : loss: 0.299864, loss_dice: 0.306806, loss_ce: 0.292922
[10:08:41.021] iteration 165 : loss: 0.335537, loss_dice: 0.333436, loss_ce: 0.337638
[10:08:42.607] iteration 166 : loss: 0.361997, loss_dice: 0.358700, loss_ce: 0.365294
[10:08:44.152] iteration 167 : loss: 0.280011, loss_dice: 0.282915, loss_ce: 0.277108
[10:08:45.711] iteration 168 : loss: 0.287389, loss_dice: 0.285628, loss_ce: 0.289150
[10:08:47.330] iteration 169 : loss: 0.349689, loss_dice: 0.369816, loss_ce: 0.329562
[10:08:48.963] iteration 170 : loss: 0.376769, loss_dice: 0.368264, loss_ce: 0.385274
[10:08:50.522] iteration 171 : loss: 0.263087, loss_dice: 0.273619, loss_ce: 0.252554
[10:08:52.069] iteration 172 : loss: 0.300292, loss_dice: 0.295814, loss_ce: 0.304770
[10:08:53.618] iteration 173 : loss: 0.389954, loss_dice: 0.401359, loss_ce: 0.378548
[10:08:55.155] iteration 174 : loss: 0.240088, loss_dice: 0.258676, loss_ce: 0.221499
[10:08:56.718] iteration 175 : loss: 0.437514, loss_dice: 0.383385, loss_ce: 0.491643
[10:08:58.255] iteration 176 : loss: 0.239175, loss_dice: 0.259621, loss_ce: 0.218728
[10:08:59.844] iteration 177 : loss: 0.361429, loss_dice: 0.350995, loss_ce: 0.371863
[10:09:01.499] iteration 178 : loss: 0.287108, loss_dice: 0.326704, loss_ce: 0.247512
[10:09:03.081] iteration 179 : loss: 0.308787, loss_dice: 0.333138, loss_ce: 0.284437
[10:09:04.639] iteration 180 : loss: 0.381459, loss_dice: 0.380217, loss_ce: 0.382702
[10:09:06.190] iteration 181 : loss: 0.229271, loss_dice: 0.246887, loss_ce: 0.211656
[10:09:07.743] iteration 182 : loss: 0.336656, loss_dice: 0.356232, loss_ce: 0.317081
[10:09:09.305] iteration 183 : loss: 0.208159, loss_dice: 0.233109, loss_ce: 0.183208
[10:09:10.847] iteration 184 : loss: 0.255920, loss_dice: 0.264667, loss_ce: 0.247172
[10:09:12.400] iteration 185 : loss: 0.266026, loss_dice: 0.267747, loss_ce: 0.264304
[10:09:14.077] iteration 186 : loss: 0.266687, loss_dice: 0.285324, loss_ce: 0.248051
[10:09:15.728] iteration 187 : loss: 0.395229, loss_dice: 0.403027, loss_ce: 0.387431
[10:09:17.277] iteration 188 : loss: 0.306762, loss_dice: 0.314700, loss_ce: 0.298824
[10:09:18.825] iteration 189 : loss: 0.371039, loss_dice: 0.364497, loss_ce: 0.377581
[10:09:20.384] iteration 190 : loss: 0.291449, loss_dice: 0.303515, loss_ce: 0.279384
[10:09:21.924] iteration 191 : loss: 0.370836, loss_dice: 0.371888, loss_ce: 0.369784
[10:09:23.475] iteration 192 : loss: 0.329227, loss_dice: 0.340333, loss_ce: 0.318121
[10:09:25.022] iteration 193 : loss: 0.323742, loss_dice: 0.343340, loss_ce: 0.304144
[10:09:26.656] iteration 194 : loss: 0.257457, loss_dice: 0.276261, loss_ce: 0.238652
[10:09:28.289] iteration 195 : loss: 0.301849, loss_dice: 0.332603, loss_ce: 0.271095
[10:09:29.849] iteration 196 : loss: 0.304273, loss_dice: 0.285365, loss_ce: 0.323180
[10:09:31.403] iteration 197 : loss: 0.369032, loss_dice: 0.367973, loss_ce: 0.370091
[10:09:32.954] iteration 198 : loss: 0.304952, loss_dice: 0.334151, loss_ce: 0.275754
[10:09:34.511] iteration 199 : loss: 0.226043, loss_dice: 0.239737, loss_ce: 0.212348
[10:09:36.066] iteration 200 : loss: 0.306053, loss_dice: 0.320133, loss_ce: 0.291972
